{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do': 0.1794871794871795, 'i': 0.39622641509433965, 'like': 0.30434782608695654, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01, 'am': 0.99, 'spam': 0.99, 'not': 0, 'that': 0, 'spamiam': 0}\n",
      "Spam probability: 0.0028917084685748016\n",
      "Spam probability: 0.30102389078498265\n",
      "Spam probability: 0.9998979800040808\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy\n",
    "\n",
    "# Define the corpuses\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"], [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "def make_spam_filter(good_corpus, bad_corpus):\n",
    "    # Flatten the lists and lowercase the tokens in both corpuses\n",
    "    good_corpus_flat = list(map(lambda x: x.lower(), numpy.concatenate(good_corpus)))\n",
    "    bad_corpus_flat = list(map(lambda x: x.lower(), numpy.concatenate(bad_corpus)))\n",
    "    # Create the occurence hashes for each corpus\n",
    "    good_map = Counter(good_corpus_flat)\n",
    "    bad_map = Counter(bad_corpus_flat)\n",
    "\n",
    "    # Token probability function taken from reading\n",
    "    def calc_token_probability(token):\n",
    "        g = 2 * (good_map.get(token) or 0)\n",
    "        b = bad_map.get(token) or 0\n",
    "        ngood = len(good_map)\n",
    "        nbad = len(bad_map)\n",
    "        if g + b > 1:\n",
    "            return max(0.01, min(0.99, min(1.0, b / nbad) / \n",
    "                                 (min(1.0, g / ngood) + min(1.0, b/ nbad))))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Table of all the token probabilities\n",
    "    token_probability = dict(map(\n",
    "        lambda x: (x, calc_token_probability(x)), good_corpus_flat + bad_corpus_flat\n",
    "    ))\n",
    "    # Show the probability table\n",
    "    print(token_probability)\n",
    "    \n",
    "    # Return the token probability or neutral\n",
    "    def get_token_probability(token):\n",
    "        return token_probability.get(token) or 0.5\n",
    "    \n",
    "    # Probability funtion from the reading\n",
    "    def spam_probability(message):\n",
    "        probs = list(map(get_token_probability, message))\n",
    "        prod = numpy.prod(probs)\n",
    "        return prod / (prod + numpy.prod(list(map(lambda x: 1 - x, probs))))\n",
    "        \n",
    "    \n",
    "    return spam_probability\n",
    "\n",
    "# Create the spam function\n",
    "spam_prob = make_spam_filter(ham_corpus, spam_corpus)\n",
    "\n",
    "print(\"Spam probability:\", spam_prob(\"i like ham\".split()))\n",
    "print(\"Spam probability:\", spam_prob(\"i am ham i am eggs\".split()))\n",
    "print(\"Spam probability:\", spam_prob(\"I am spam spamiam\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem makes use of the Bayes rule where we calculate the probability of the cause given the effect. The email is the effect, and it is used to measure the probability of the cause which could be spam or not spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.517, True: 0.483\n",
      "False: 0.912, True: 0.088\n",
      "False: 0.957, True: 0.043\n",
      "False: 0.014, True: 0.986\n",
      "False: 0.654, True: 0.346\n"
     ]
    }
   ],
   "source": [
    "# a.\n",
    "from probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask\n",
    "\n",
    "T, F = True, False\n",
    "\n",
    "weather = BayesNet([\n",
    "    ('Cloudy', '', 0.5),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.1, F: 0.5}),\n",
    "    ('Rain', 'Cloudy', {T: 0.8, F: 0.2}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): 0.99, (T, F): 0.9, (F, T): 0.9, (F, F): 0})\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The full joint probability includes 4 random variables, so the joint distribution would contain 2x2x2x2 or 16 entries.\n",
    "\n",
    "c. There are only two independent value in the Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False: 0.52, True: 0.48\n",
      "False: 0.879, True: 0.121\n",
      "False: 0.951, True: 0.049\n",
      "False: 0.013, True: 0.987\n",
      "False: 0.641, True: 0.359\n"
     ]
    }
   ],
   "source": [
    "# d.\n",
    "\n",
    "print(gibbs_ask('Cloudy', dict(), weather).show_approx())\n",
    "print(gibbs_ask('Sprinkler', dict(Cloudy=T), weather).show_approx())\n",
    "print(gibbs_ask('Cloudy', dict(Sprinkler=T, Rain=F), weather).show_approx())\n",
    "print(gibbs_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), weather).show_approx())\n",
    "print(gibbs_ask('Cloudy', dict(WetGrass=F), weather).show_approx())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
